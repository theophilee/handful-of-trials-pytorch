{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "import gym\n",
    "from model_free import TD3\n",
    "\n",
    "class ActionRepeat(object):\n",
    "    def __init__(self, env, amount):\n",
    "        self._env = env\n",
    "        self._amount = amount\n",
    "        self._env._max_episode_steps = self._env._max_episode_steps // amount\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(self._amount):\n",
    "            obs, reward, _, _ = self._env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        return obs, total_reward, False, {}\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self._env.reset(*args, **kwargs)\n",
    "\n",
    "env = gym.make('MyHalfCheetah-v2')\n",
    "env = ActionRepeat(env, 4)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "                   \n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load('MyHalfCheetah-v2', 'save/TD3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_rollout_stats(obs, acts, reward_sum):\n",
    "    print(\"Cumulative reward \", reward_sum)\n",
    "    print(\"Action min {}, max {}, mean {}, std {}\".format(\n",
    "        acts.min(), acts.max(), acts.mean(), acts.std()))\n",
    "    print(\"Obs min {}, max {}, mean {}, std {}\".format(\n",
    "        obs.min(), obs.max(), obs.mean(), obs.std()))\n",
    "\n",
    "def sample_rollout(env, policy):\n",
    "    observations, actions, reward_sum = [env.reset()], [], 0\n",
    "\n",
    "    for t in range(env._max_episode_steps):\n",
    "        actions.append(policy.act(observations[t]))\n",
    "        obs, reward, _, _ = env.step(actions[t])\n",
    "        observations.append(obs)\n",
    "        reward_sum += reward\n",
    "\n",
    "    return np.array(observations), np.array(actions), reward_sum\n",
    "    \n",
    "    \n",
    "O, A = [], []\n",
    "for _ in range(20):\n",
    "    obs, acts, reward_sum = sample_rollout(env, policy)\n",
    "    O.append(obs)\n",
    "    A.append(acts)\n",
    "                   \n",
    "O, A = np.array(O), np.array(A)\n",
    "np.save('TD3_obs.npy', O)\n",
    "np.save('TD3_act.npy', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 251, 18) (20, 250, 6)\n",
      "-14.181554903081388 14.709950892518957 0.7288564677261283 2.1870828065655603\n",
      "-1.0 1.0 -0.44035676 0.8007284\n"
     ]
    }
   ],
   "source": [
    "O, A = np.load('TD3_obs.npy'), np.load('TD3_act.npy')\n",
    "print(O.shape, A.shape)\n",
    "print(O.min(), O.max(), O.mean(), O.std())\n",
    "print(A.min(), A.max(), A.mean(), A.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.03721834, -0.1751069 , -0.12655779,  0.48382039,  0.55996988,\n",
      "        0.49546049,  0.71287399,  0.62377733,  0.51271492, -0.02325877,\n",
      "       -1.46481013,  0.91993719, -2.76050593, -3.01385295, -2.40930781,\n",
      "       -0.207205  , -5.03486061, -0.07091486]), 0.06025153047648446, False, {})\n",
      "(array([-0.03721834, -0.1751069 , -0.12655779,  0.48382039,  0.55996988,\n",
      "        0.49546049,  0.71287399,  0.62377733,  0.51271492, -0.02325877,\n",
      "       -1.46481013,  0.91993719, -2.76050593, -3.01385295, -2.40930781,\n",
      "       -0.207205  , -5.03486061, -0.07091486]), -7.139748469523517, False, {})\n",
      "(array([-0.00453046, -0.14839396, -0.06351854,  0.26520561,  0.27608194,\n",
      "        0.25260678,  0.44108708,  0.33240966,  0.33058094, -0.06338864,\n",
      "       -1.61868918,  0.54272409, -2.30551309, -1.5785326 , -1.35781413,\n",
      "       -1.29138198, -0.95469576, -0.8698062 ]), 1.3223125350474048, False, {})\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "state = env.sim.get_state()\n",
    "env.sim.set_state(state)\n",
    "print(env.step(np.ones(env.action_space.shape)))\n",
    "env.sim.set_state(state)\n",
    "print(env.step(np.ones(env.action_space.shape) * 2))\n",
    "env.sim.set_state(state)\n",
    "print(env.step(np.ones(env.action_space.shape) * 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.load('expert_demonstrations/half_cheetah/expert_obs.npy')\n",
    "A = np.load('expert_demonstrations/half_cheetah/expert_act.npy')\n",
    "print(O.shape, A.shape)\n",
    "print(O.min(), O.max(), O.mean(), O.std())\n",
    "print(A.min(), A.max(), A.mean(), A.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpi-global)",
   "language": "python",
   "name": "dpi-global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
