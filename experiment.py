import os
import time
import torch
import numpy as np

from utils import Logger


class Experiment:
    def __init__(self, mpc, policy, logdir, savedir, args):
        """Experiment.

        Arguments:
            mpc (MPC): Model-predictive controller containing dynamics model
                to be trained.
            policy (Policy): Parameterized reactive policy to be trained by
                imitation learning on model-based controller.
            logdir: Log directory for Tensorboard.
            savedir:
            args (DotMap): A DotMap of experiment parameters.
                .env: (OpenAI gym environment) The environment for this agent.
                .task_hor (int): Task horizon.
                .num_rollouts (int): Number of rollouts for which we train.
                .num_imagined_rollouts (int): Number of imagined rollouts per
                    iteration of inner imitation learning loop.
        """
        self.mpc = mpc
        self.policy = policy
        self.env = args.env
        self.task_hor = args.task_hor
        self.num_rollouts = args.num_rollouts
        self.num_imagined_rollouts = args.num_imagined_rollouts

        self.path_mpc = os.path.join(savedir, 'mpc.pth')
        self.path_policy = os.path.join(savedir, 'policy.pth')

        # Tensorboard summary writer
        self.logger = Logger(logdir)

    def run_baseline(self):
        """Simple model predictive control baseline (no parameterized policy).
        """
        # Initial random rollout
        obs, acts, reward_sum = self.sample_rollout(actor='mpc')
        self.mpc.train(obs, acts)

        # Training loop
        for i in range(self.num_rollouts):
            print()
            print("Starting training iteration %d." % (i + 1))

            # Sample rollout from mpc
            obs_mpc, acts_mpc, reward_sum_mpc = self.sample_rollout(actor='mpc')

            # Train model
            metrics_model, weights_model, grads_model = self.mpc.train(obs_mpc, acts_mpc)

            print("MPC cumulative reward ", reward_sum_mpc)
            print("MPC action min {}, max {}, mean {}, std {}".format(
                acts_mpc.min(), acts_mpc.max(), acts_mpc.mean(), acts_mpc.std()))

            # Log to Tensorboard
            step = (i + 1) * self.task_hor
            self.logger.log_scalar("reward/mpc", reward_sum_mpc, step)

            for key, metric in metrics_model.items():
                self.logger.log_scalar("{}/mean".format(key), metric.mean(), step)
            #    for n in range(len(metric)):
            #        self.logger.log_scalar("{}/model{}".format(key, n + 1), metric[n], step)

            # for key, weight in weights_model.items():
            #   self.logger.log_histogram("weight/{}".format(key), weight, step)
            # for key, grad in grads_model.items():
            #   self.logger.log_histogram("grad/{}".format(key), grad, step)

            # Save model
            torch.save(self.mpc, self.path_mpc)

    def run_experiment(self, algo):
        """Learn parameterized policy by behavior cloning on trajectories generated by
        model-predictive controller under the approximate model.

        Argument:
            algo (str): one of 'behavior_cloning', 'dagger'
        """
        assert algo in ['behavior_cloning', 'dagger']

        # Initial random rollout
        obs, acts, reward_sum = self.sample_rollout(actor='mpc')
        self.mpc.train(obs, acts)

        # Training loop
        for i in range(self.num_rollouts):
            print("\nStarting training iteration %d." % (i + 1))

            # Reset policy training dataset
            self.policy.reset_training_set()

            while True:
                if algo == 'behavior_cloning':
                    # Generate imaginary rollouts with the controller
                    obs_imagined, acts_imagined = self.mpc.sample_imaginary_rollouts(
                        actor='mpc', task_hor=self.task_hor, num_rollouts=self.num_imagined_rollouts)

                if algo == 'dagger':
                    raise NotImplementedError

                    # Generate imaginary rollouts with the policy
                    #obs_imagined, _ = self.mpc.sample_imaginary_rollouts(
                    #    actor='policy', task_hor=self.task_hor, num_rollouts=self.num_imagined_rollouts)

                    # Label rollouts with the controller
                    #acts_imagined = self.mpc.label(obs_imagined[:-1])

                # Train parameterized policy by behavior cloning
                metrics_policy = self.policy.train(obs_imagined.reshape(-1, obs_imagined.shape[-1]),
                                                   acts_imagined.reshape(-1, acts_imagined.shape[-1]))

                # TODO imagined rollouts are completely crazy compared to true rollouts
                # e.g min -7.509242057800293, max 46.9343376159668, mean 5.970691697410477, std 11.511027530038433
                # vs. min - 5.286344230310194, max 10.152343352297112, mean - 0.5799591895719013, std 1.45819241183518

                print("Imitation learning test error", metrics_policy["policy/mse/test"])
                print("Imitation learning training error", metrics_policy["policy/mse/train"])
                print("Imagined rollout obs min {}, max {}, mean {}, std {}".format(
                    obs_imagined.min(), obs_imagined.max(), obs_imagined.mean(), obs_imagined.std()))
                print("Imagined rollout act min {}, max {}, mean {}, std {}".format(
                    acts_imagined.min(), acts_imagined.max(), acts_imagined.mean(), acts_imagined.std()))

                if metrics_policy["policy/mse/test"] < 1.0:
                    break

            # Sample rollout from policy
            obs_policy, acts_policy, reward_sum_policy = self.sample_rollout(actor='policy')

            # Train model
            metrics_model, weights_model, grads_model = self.mpc.train(obs_policy, acts_policy)

            print("Policy cumulative reward ", reward_sum_policy)
            print("True obs min {}, max {}, mean {}, std {}".format(
                obs_policy.min(), obs_policy.max(), obs_policy.mean(), obs_policy.std()))
            print("Policy action min {}, max {}, mean {}, std {}".format(
                acts_policy.min(), acts_policy.max(), acts_policy.mean(), acts_policy.std()))

            # Log to Tensorboard
            step = (i + 1) * self.task_hor
            self.logger.log_scalar("reward/policy", reward_sum_policy, step)

            #for key, metric in metrics_policy.items():
            #    self.logger.log_scalar(key, metric, step)

            for key, metric in metrics_model.items():
                self.logger.log_scalar("{}/mean".format(key), metric.mean(), step)

    def run_debug(self):
        """Train behaviour cloning / DAgger in a simpler non-iterative setting to get a feel
        for it and get hyper-parameters right. Imitate fixed nearly optimal policy (e.g
        trained controller).
        """
        # Restore model
        self.mpc = torch.load(self.path_mpc)

        # Training loop
        for i in range(self.num_rollouts):
            print()
            print("Starting training iteration %d." % (i + 1))

            # Sample rollout from mpc
            obs_mpc, acts_mpc, reward_sum_mpc = self.sample_rollout(actor='mpc')
            print("MPC cumulative reward ", reward_sum_mpc)
            print("MPC rollout act min {}, max {}, mean {}, std {}".format(
                acts_mpc.min(), acts_mpc.max(), acts_mpc.mean(), acts_mpc.std()))

            # Train parameterized policy by behavior cloning
            metrics_policy = self.policy.train(obs_mpc[:-1], acts_mpc)
            print("Imitation learning test error", metrics_policy["policy/mse/test"])
            print("Imitation learning training error", metrics_policy["policy/mse/train"])

            # Sample rollout from parameterized policy for evaluation
            obs_policy, acts_policy, reward_sum_policy = self.sample_rollout(actor='policy')
            print("Policy cumulative reward ", reward_sum_policy)
            print("Policy rollout act min {}, max {}, mean {}, std {}".format(
                acts_policy.min(), acts_policy.max(), acts_policy.mean(), acts_policy.std()))


    def sample_rollout(self, actor):
        """Sample a rollout generated by a given actor in the environment.

        Argument:
            actor (str): One of 'mpc', 'policy'.

        Returns:
            obs (1D numpy.ndarray): Trajectory of observations.
            acts (1D numpy.ndarray): Trajectory of actions.
            reward_sum (int): Sum of accumulated rewards.
        """
        assert actor in ['mpc', 'policy']
        O, A, reward_sum, done, times = [self.env.reset()], [], 0, False, []

        if actor == 'mpc':
            self.mpc.reset()

        for t in range(self.task_hor):
            start = time.time()

            if actor == 'mpc':
                A.append(self.mpc.act(O[t]))
            else:
                A.append(self.policy.act(O[t]))

            times.append(time.time() - start)

            obs, reward, done, info = self.env.step(A[t])

            O.append(obs)
            reward_sum += reward

            if done:
                break

        #print("Average action selection time: ", np.mean(times))

        return np.array(O), np.array(A), reward_sum